# Language Modeling Assignment

## ğŸ“˜ Overview

This project focuses on building and evaluating a statistical language model using **N-Grams**. You'll explore the estimation of sentence probabilities, handle unknown words, and apply smoothing techniques to improve model performance.

---

## ğŸ“š Concepts Covered

- **N-Grams and Probabilities**  
  Calculate unigram, bigram, and trigram probabilities.

- **Approximate Sentence Probability**  
  Use N-Gram models to estimate the likelihood of sentences.

- **Building a Language Model**  
  Train language models from a corpus using N-Gram statistics.

- **Fixing Missing Information**  
  Address incomplete N-Grams through smoothing techniques.

- **Out-of-Vocabulary Words (`<UNK>`)**  
  Replace rare or unknown tokens with `<UNK>`.

- **Smoothing, Backoff, and Interpolation**  
  Implement:
  - Laplace (Add-One) Smoothing  
  - Backoff strategies  
  - Interpolation techniques

- **Evaluation with Perplexity**  
  Measure model performance using perplexity scores.

---

## ğŸ“˜ Overview

This project demonstrates how to build a statistical language model using **N-Grams** from a corpus. It covers computing sentence probabilities, handling unknown words, applying smoothing techniques, and evaluating the model using perplexity.

---

## ğŸ“š Key Concepts and Simple Definitions

### ğŸ”¢ 1. N-Grams and Probabilities
An **N-Gram** is a sequence of N words.  
- **Unigram**: 1 word (e.g., "language")  
- **Bigram**: 2 words (e.g., "language model")  
- **Trigram**: 3 words (e.g., "build a model")  

These help compute how likely a word is to follow another.

---

### ğŸ“ 2. Approximate Sentence Probability from N-Grams
We estimate the probability of a sentence using N-Grams.  
Example (Bigram Model):  
`P("I love NLP") â‰ˆ P(I) * P(love | I) * P(NLP | love)`

---

### ğŸ—ï¸ 3. Build a Language Model from a Corpus
A **language model** assigns probabilities to sequences of words.  
We build it by:
- Counting N-Grams in a **corpus** (text dataset)
- Calculating probabilities from these counts

---

### âš ï¸ 4. Fix Missing Information
Some word combinations might not appear in the training data.  
We **fix this** so the model doesn't assign zero probability by using:
- Smoothing
- Backoff
- Interpolation

---

### â“ 5. Out-of-Vocabulary (OOV) Words with `<UNK>`
Words not seen in training are called **out-of-vocabulary**.  
We replace them with a special token: `<UNK>` (unknown word)  
This helps the model handle new words in test data.

---

### ğŸ› ï¸ 6. Missing N-Gram Handling with:
#### ğŸ”¹ Smoothing
Adds a small count (e.g., +1) to all possible N-Grams to avoid zero probability.  
**Example**: Laplace Smoothing

#### ğŸ”¹ Backoff
If a higher-order N-Gram is missing, **fall back** to a lower-order one.  
(e.g., Use bigram if trigram is missing)

#### ğŸ”¹ Interpolation
**Combine** probabilities from multiple N-Gram models (e.g., trigram + bigram + unigram) using weighted averages.

---

### ğŸ“Š 7. Evaluate Language Model with Perplexity
**Perplexity** measures how well the model predicts a sample of text.  
- **Lower perplexity = better model**
- Think of it as the modelâ€™s â€œsurprise levelâ€ on new data.

Formula:  
Perplexity = (1 / P(sentence))^(1/N)

---

## ğŸ› ï¸ Features

- Tokenization and preprocessing
- Support for unigram, bigram, and trigram models
- Rare word handling with `<UNK>`
- Add-One Smoothing
- Backoff and interpolation methods
- Perplexity-based evaluation
---